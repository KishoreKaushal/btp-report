\chapter{PIDForest}
\label{ch:pidforest}

In chapter \ref{ch:introduction} we discussed some of the challenges that an anomaly detection algorithm face. 
In the previous chapter we discussed an isolation based ensemble method which is a very good method in terms of complexity and accuracy. Isolation forest tried to address the problems related to masking and swamping.
In this chapter we will point out some major issues with isolation forest and 
discuss another ensemble method, which improves upon those issues.

\section{Issues with Isolation Forest}
\label{sec:issues-with-iforest}

\paragraph{Random Split:} iForest repeatedly samples a set $X'$ of $\psi$ points from $X$ and builds a random tree with those points as leaves. 
The tree is built by choosing a random co-ordinate $q$, and a random value $p$ in its range about which to split. 
Since iForest chooses which coordinate we split on as well as the breakpoint at random. 
Thus to be isolated at small depth frequently, picking splits at random must have a good chance of isolating an anomalous point.
Although, there are other variants like extended isolation forest which improves on this, but there is no significant improvements.

\paragraph{High Dimensional:} As the number of co-ordinates or attributes increases, probability of choosing a sequence of attributes for split which gives rise to most of the anomalies will be very less. 
Hence, it is very likely that anomalous points won't be isolated near the root and false negative cases will increase. (In anomaly detection problem, anomaly is the true class.)

\paragraph{Presence of non-ordinal categorical attributes:} A very big limitation of iForest is that it only works for those datasets where all the features are real-values or ordinal.

\section{Partial Indentification}
\label{sec:partial-identification}

We will briefly discuss some concepts present in section 2 of Vatsal et al. \cite{NIPS2019_9710}. 

\paragraph{Notations:} Let $T$ denote a dataset of $n$ points in $d$ dimensions. Given indices $S \subseteq [d]$ and $x \in R$, let $x_S$ denote the projection of $x$ onto coordinates in $S$. All the logarithms are to base 2.

\subsection{Boolean Setting}
\label{subsec:boolean-setting}

In Boolean setting $T \subseteq \{0,1\}^d$ and assume that $T$ has no duplicates. 

\begin{defn}
    \label{defn:id-for-a-point}
    (ID for a point)
    \vspace{-1em}
    \begin{equation}
        \label{eq:ID}
        \begin{split}
        id = \{ S \mid S \subseteq [d], x \in T \textrm{ and } \forall y \in T \setminus x, \, x_S \neq y_S \}  \\
        ID(x,T) = arg \min_{S \subseteq [d]} \vert \{ S \mid S \subseteq [d], x \in T \textrm{ and } \forall y \in T \setminus x, \, x_S \neq y_S \} \vert
        \end{split}
    \end{equation}
    \vspace{-2em}
    \begin{equation}
        idLength(x,T) = \vert ID(x, T) \vert
    \end{equation}
\end{defn}

\begin{defn}
    \label{defn:imposters}
    (Imposters) 
    \vspace{-1em}
    \begin{equation}
        \label{eq:imposters}
        Imp(x, T, S) = \{y \in T \mid x \in T, S \subseteq [d] \textrm{ and } x_S = y_S\}
    \end{equation}
\end{defn}


\begin{defn}
    \label{defn:partial-ids}
    (Partial ID) 
    \vspace{-1em}
    \begin{equation}
        \label{eq:PID}
        PID(x,T) = arg \min_{S \subseteq [d]} 
    (\vert S \vert + log_{2}(\vert Imp(x, T, S) \vert)),
    \end{equation}
    \vspace{-2em}
    \begin{equation}
        \label{eq:pidLength}
        pidLength(x,T) = \min_{S \subseteq [d]} 
    (\vert S \vert + log_{2}(\vert Imp(x, T, S) \vert)).
    \end{equation}
\end{defn}


\paragraph{Geometric view of pidLength: } 
A subcube $C$ of $\{0, 1\}^d$ is the set of points obtained by fixing some subset $S \subseteq [d]$ coordinates to values in 0, 1. (Refer section 1 of \cite{ellis_2011})
The sparsity of $T$ in a subcube C is $\rho_{0,1}(T, C) = \frac{\vert C \vert}{\vert C \cap T\vert}$. 
The notation $C \ni x$ means that $C$ contains $x$, hence $\min_{C \ni x}$ is the minimum over all $C$ that contain $x$. Anomalies are points that lie in relatively sparse subcubes. Low scores come with a natural witness: a sparse subcube $PID(x, T)$ containing relatively few points from T.

\begin{defn}
    \label{defn:pidforest-anomaly-score}
    (Anomaly Score)
    \vspace{-1em}
    \begin{equation}
        \label{eq:pidforest-anomaly-score}
        s(x,T) = 2^{-pidLength(x,T)}
    \end{equation}
\end{defn}

\subsection{Continuous Setting}

Without loss of generality assume that $T \subseteq [0,1]^d$. 
Length of an interval $I = [a,b], 0 \leq a \leq b \leq 1$ is $len(I) = (b-a)$. 
A subcube $C$ is specified by a subset of co-ordinates $S$ and intervals $I_j, \forall j \in S$. 
It consists of all points such that $x_j \in I_j, \forall j \in S$. 

\begin{defn}
    \label{defn:volume-of-subcube}
    (Volume of a Subcube)
    \vspace{-1em}
    \begin{equation}
        \label{eq:volume-of-subcube}
        vol(C) = \prod_i len(I_i) \textrm{ where } C = \prod_j I_j \textrm{ and } I_k = [0,1] \textrm{ for } k \notin S.
    \end{equation}
\end{defn}

\begin{defn}
    \label{defn:sparsity}
    (Sparsity of T in C)
    \vspace{-1em}
    \begin{equation}
        \label{eq:sparsity}
        \rho (T,C) = \frac{vol(C)}{\vert C \cap T \vert}
    \end{equation}
\end{defn}

\pagebreak

\begin{defn}
    \label{defn:pidscore}
    (PIDScore of x in T)
    \vspace{-1em}
    \begin{equation}
        \label{eq:pidscore}
        \begin{split}
            PIDScore(x,T) = \max_{C \ni x} \rho (T,C), \\
            PID(x,T) = arg \max_{C \ni x} \rho (T,C).
        \end{split}
    \end{equation}
\end{defn}

Refer section 2.2 of Vatsal et al. \cite{NIPS2019_9710} to see the analogy to the Boolean case.

\subsection{Other attributes}

To handle attributes over a domain D, we need to specify what subsets of D are intervals and how we measure their length.
For discrete attributes, it is natural to define $len(I) = \frac{\vert I \vert}{\vert D \vert}$.
For unordered discrete values, the right definition of interval could be singleton sets, like $country = Brazil$ or certain subsets, like $continent = Americas$. The right choice will depend on the dataset.


\section{PIDForest Algorithm}
\label{sec:pidforest-algorithm}

Like with iForest, the PIDForest algorithm builds an ensemble of decision trees, each tree is built using a sample of the data set and partitions the space into subcubes. 
However, the way the trees are constructed and the criteria by which a point is declared anomalous are very different.

Vatsal et al. \cite{NIPS2019_9710} provided a rough idea on how a PIDForest is to be constructed without emphasising much on the pseudo-code. In this report we will fill the gaps and provide pseudo-codes for various data structures to be implemented for PIDForest.

Each node of a tree corresponds to a subcube $C$, the children of $C$ represent a disjoint partition of $C$ along some axis $i \in [d]$ (iTree$^{[\ref{alg:iTree}]}$ always splits C into two, here finer partition is allowed). 
The goal is to have large variance in the sparsity ($\rho$) of the subcubes. 
Ultimately, the leaves with large sparsity values will point to regions with anomalies.

For each tree, we pick a random sample $P \subseteq T$ of $m$ points, and use that subset to build the tree.
Each node $v$ in the tree corresponds to subcube $C(v)$, and a set of points $P(v) = C(v) \cap P$.
For the root, $C(v) = [0, 1]^d$ and $P (v) = P$. 
At each internal node, we pick a coordinate $j \in [d]$, and breakpoints $t_{1} \leq ... \leq t_{k-1}$ which partition $I_j$ into $k$ intervals, and split $C$ into $k$ subcubes. 

\textbf{How to choose the partition?} We want to partition the cube into some sparse regions and some dense regions. 
This idea is formulized in section 3 of Vatsal et al. \cite{NIPS2019_9710} and the objective functions turns out to be:

\vspace{-1em}
\begin{equation}
    \label{eq:maximizing-variance}
    % \begin{split}
        arg \max_{ \{ C^i \}_{i \in k} } Var(P, \{ C^i \}_{i \in k})
    % \end{split}
\end{equation}

Maximizing the variance has the advantage that it turns out to equivalent to a well-studied problem about histograms, and admits a very efficient streaming algorithm. Here we are going to use $(1+ \epsilon)-$factor approximation algorithm for histogram construction given by Guha et al. \cite{10.1145/1132863.1132873}.