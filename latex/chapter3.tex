\chapter{PIDForest}
\label{ch:pidforest}

In chapter \ref{ch:introduction} we discussed some of the challenges that an anomaly detection algorithm face. 
In the previous chapter we discussed an isolation based ensemble method which is a very good method in terms of complexity and accuracy. Isolation forest tried to address the problems related to masking and swamping.
In this chapter we will point out some major issues with isolation forest and 
discuss another ensemble method, which improves upon those issues.

\section{Issues with Isolation Forest}
\label{sec:issues-with-iforest}

\paragraph{Random Split:} iForest repeatedly samples a set $X'$ of $\psi$ points from $X$ and builds a random tree with those points as leaves. 
The tree is built by choosing a random co-ordinate $q$, and a random value $p$ in its range about which to split. 
Since iForest chooses which coordinate we split on as well as the breakpoint at random. 
Thus to be isolated at small depth frequently, picking splits at random must have a good chance of isolating an anomalous point.
Although, there are other variants like extended isolation forest which improves on this, but there is no significant improvements.

\paragraph{High Dimensional:} As the number of co-ordinates or attributes increases, probability of choosing a sequence of attributes for split which gives rise to most of the anomalies will be very less. 
Hence, it is very likely that anomalous points won't be isolated near the root and false negative cases will increase. (In anomaly detection problem, anomaly is the true class.)

\paragraph{Presence of non-ordinal categorical attributes:} A very big limitation of iForest is that it only works for those datasets where all the features are real-values or ordinal.

\section{Partial Indentification}
\label{sec:partial-identification}

We will briefly discuss some concepts present in section 2 of Vatsal et al. \cite{NIPS2019_9710}. 

\paragraph{Notations:} Let $T$ denote a dataset of $n$ points in $d$ dimensions. Given indices $S \subseteq [d]$ and $x \in R$, let $x_S$ denote the projection of $x$ onto coordinates in $S$. All the logarithms are to base 2.

\subsection{Boolean Setting}
\label{subsec:boolean-setting}

In Boolean setting $T \subseteq \{0,1\}^d$ and assume that $T$ has no duplicates. 

\begin{defn}
    (IDs for a point) We say that $S \subseteq [d]$ is an ID for $x \in T$ if $x_S \neq y_S\,, \forall y \in T \setminus {x}$.
    Let ID(x,T) be the smallest ID for $x$ breaking ties arbitrarily. 
    Let idLength(x,T) = $\vert ID(x, T) \vert$.
\end{defn}

\begin{defn}
    (Imposters)  Given $x \in T$ and $S \subseteq [d]$, the set of   impostors of $x$ in $T$ are all points that equal $x$ on all coordinates in $S$, i.e., $Imp(x, T, S) = \{y \in T \mid x_S = y_S\}$.
\end{defn}

\begin{defn}
    (Partial ID) 
    $PID(x,T) = arg \min_{S \subseteq [d]} 
    (\vert S \vert + log_{2}(\vert Imp(x, T, S) \vert)),$

    and $pidLength(x,T) = \min_{S \subseteq [d]} 
    (\vert S \vert + log_{2}(\vert Imp(x, T, S) \vert)).$
\end{defn}
\pagebreak

\paragraph{Geometric view of pidLength: } 
A subcube $C$ of $\{0, 1\}^d$ is the set of points obtained by fixing some subset $S \subseteq [d]$ coordinates to values in 0, 1. 
The sparsity of $T$ in a subcube C is $\rho_{0,1}(T, C) = \frac{\vert C \vert}{\vert C \cap T\vert}$. 
The notation $C \ni x$ means that $C$ contains $x$, hence $\min_{C \ni x}$ is the minimum over all $C$ that contain $x$. Anomalies are points that lie in relatively sparse subcubes. Low scores come with a natural witness: a sparse subcube $PID(x, T)$ containing relatively few points from T.
