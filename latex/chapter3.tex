\chapter{PIDForest}
\label{ch:pidforest}

In chapter \ref{ch:introduction} we discussed some of the challenges that an anomaly detection algorithm face. 
In the previous chapter we discussed an isolation based ensemble method which is a very good method in terms of complexity and accuracy. Isolation forest tried to address the problems related to masking and swamping.
In this chapter we will point out some major issues with isolation forest and 
discuss another ensemble method, which improves upon those issues.

\section{Issues with Isolation Forest}
\label{sec:issues-with-iforest}

\paragraph{Random Split:} iForest repeatedly samples a set $X'$ of $\psi$ points from $X$ and builds a random tree with those points as leaves. 
The tree is built by choosing a random co-ordinate $q$, and a random value $p$ in its range about which to split. 
Since iForest chooses which coordinate we split on as well as the breakpoint at random. 
Thus to be isolated at small depth frequently, picking splits at random must have a good chance of isolating an anomalous point.
Although, there are other variants like extended isolation forest which improves on this, but there is no significant improvements.

\paragraph{High Dimensional:} As the number of co-ordinates or attributes increases, probability of choosing a sequence of attributes for split which gives rise to most of the anomalies will be very less. 
Hence, it is very likely that anomalous points won't be isolated near the root and false negative cases will increase. (In anomaly detection problem, anomaly is the true class.)

\paragraph{Presence of non-ordinal categorical attributes:} A very big limitation of iForest is that it only works for those datasets where all the features are real-values or ordinal.

\section{Partial Indentification}
\label{sec:partial-identification}

We will briefly discuss some concepts present in section 2 of Vatsal et al. \cite{NIPS2019_9710}. 

\paragraph{Notations:} Let $T$ denote a dataset of $n$ points in $d$ dimensions. Given indices $S \subseteq [d]$ and $x \in R$, let $x_S$ denote the projection of $x$ onto coordinates in $S$. All the logarithms are to base 2.

\subsection{Boolean Setting}
\label{subsec:boolean-setting}

In Boolean setting $T \subseteq \{0,1\}^d$ and assume that $T$ has no duplicates. 

\begin{defn}
    \label{defn:id-for-a-point}
    (ID for a point)
    \vspace{-1em}
    \begin{equation}
        \label{eq:ID}
        \begin{split}
        id = \{ S \mid S \subseteq [d], x \in T \textrm{ and } \forall y \in T \setminus x, \, x_S \neq y_S \}  \\
        ID(x,T) = arg \min_{S \subseteq [d]} \vert \{ S \mid S \subseteq [d], x \in T \textrm{ and } \forall y \in T \setminus x, \, x_S \neq y_S \} \vert
        \end{split}
    \end{equation}
    \vspace{-2em}
    \begin{equation}
        idLength(x,T) = \vert ID(x, T) \vert
    \end{equation}
\end{defn}

\begin{defn}
    \label{defn:imposters}
    (Imposters) 
    \vspace{-1em}
    \begin{equation}
        \label{eq:imposters}
        Imp(x, T, S) = \{y \in T \mid x \in T, S \subseteq [d] \textrm{ and } x_S = y_S\}
    \end{equation}
\end{defn}


\begin{defn}
    \label{defn:partial-ids}
    (Partial ID) 
    \vspace{-1em}
    \begin{equation}
        \label{eq:PID}
        PID(x,T) = arg \min_{S \subseteq [d]} 
    (\vert S \vert + log_{2}(\vert Imp(x, T, S) \vert)),
    \end{equation}
    \vspace{-2em}
    \begin{equation}
        \label{eq:pidLength}
        pidLength(x,T) = \min_{S \subseteq [d]} 
    (\vert S \vert + log_{2}(\vert Imp(x, T, S) \vert)).
    \end{equation}
\end{defn}


\paragraph{Geometric view of pidLength: } 
A subcube $C$ of $\{0, 1\}^d$ is the set of points obtained by fixing some subset $S \subseteq [d]$ coordinates to values in 0, 1. (Refer section 1 of \cite{ellis_2011})
The sparsity of $T$ in a subcube C is $\rho_{0,1}(T, C) = \frac{\vert C \vert}{\vert C \cap T\vert}$. 
The notation $C \ni x$ means that $C$ contains $x$, hence $\min_{C \ni x}$ is the minimum over all $C$ that contain $x$. Anomalies are points that lie in relatively sparse subcubes. Low scores come with a natural witness: a sparse subcube $PID(x, T)$ containing relatively few points from T.

\begin{defn}
    \label{defn:pidforest-anomaly-score}
    (Anomaly Score)
    \vspace{-1em}
    \begin{equation}
        \label{eq:pidforest-anomaly-score}
        s(x,T) = 2^{-pidLength(x,T)}
    \end{equation}
\end{defn}

\subsection{Continuous Setting}

Without loss of generality assume that $T \subseteq [0,1]^d$. 
Length of an interval $I = [a,b], 0 \leq a \leq b \leq 1$ is $len(I) = (b-a)$. 
A subcube $C$ is specified by a subset of co-ordinates $S$ and intervals $I_j, \forall j \in S$. 
It consists of all points such that $x_j \in I_j, \forall j \in S$. 

\begin{defn}
    \label{defn:volume-of-subcube}
    (Volume of a Subcube)
    \vspace{-1em}
    \begin{equation}
        \label{eq:volume-of-subcube}
        vol(C) = \prod_i len(I_i) \textrm{ where } C = \prod_j I_j \textrm{ and } I_k = [0,1] \textrm{ for } k \notin S.
    \end{equation}
\end{defn}

\begin{defn}
    \label{defn:sparsity}
    (Sparsity of T in C)
    \vspace{-1em}
    \begin{equation}
        \label{eq:sparsity}
        \rho (T,C) = \frac{vol(C)}{\vert C \cap T \vert}
    \end{equation}
\end{defn}

\pagebreak

\begin{defn}
    \label{defn:pidscore}
    (PIDScore of x in T)
    \vspace{-1em}
    \begin{equation}
        \label{eq:pidscore}
        \begin{split}
            PIDScore(x,T) = \max_{C \ni x} \rho (T,C), \\
            PID(x,T) = arg \max_{C \ni x} \rho (T,C).
        \end{split}
    \end{equation}
\end{defn}

Refer section 2.2 of Vatsal et al. \cite{NIPS2019_9710} to see the analogy to the Boolean case.

\subsection{Other attributes}

To handle attributes over a domain D, we need to specify what subsets of D are intervals and how we measure their length.
For discrete attributes, it is natural to define $len(I) = \frac{\vert I \vert}{\vert D \vert}$.
For unordered discrete values, the right definition of interval could be singleton sets, like $country = Brazil$ or certain subsets, like $continent = Americas$. The right choice will depend on the dataset.


\section{PIDForest Algorithm}
\label{sec:pidforest-algorithm}

Like with iForest, the PIDForest algorithm builds an ensemble of decision trees, each tree is built using a sample of the data set and partitions the space into subcubes. 
However, the way the trees are constructed and the criteria by which a point is declared anomalous are very different.

Vatsal et al. \cite{NIPS2019_9710} provided a rough idea on how a PIDForest is to be constructed without emphasising much on the pseudo-code. In this report we will fill the gaps and provide pseudo-codes for various data structures to be implemented for PIDForest.

Each node of a tree corresponds to a subcube $C$, the children of $C$ represent a disjoint partition of $C$ along some axis $i \in [d]$ (iTree$^{[\ref{alg:iTree}]}$ always splits C into two, here finer partition is allowed). 
The goal is to have large variance in the sparsity ($\rho$) of the subcubes. 
Ultimately, the leaves with large sparsity values will point to regions with anomalies.

For each tree, we pick a random sample $P \subseteq T$ of $m$ points, and use that subset to build the tree.
Each node $v$ in the tree corresponds to subcube $C(v)$, and a set of points $P(v) = C(v) \cap P$.
For the root, $C(v) = [0, 1]^d$ and $P (v) = P$. 
At each internal node, we pick a coordinate $j \in [d]$, and breakpoints $t_{1} \leq ... \leq t_{k-1}$ which partition $I_j$ into $k$ intervals, and split $C$ into $k$ subcubes. 

\textbf{How to choose the partition?} We want to partition the cube into some sparse regions and some dense regions. 
This idea is formulized in section 3 of Vatsal et al. \cite{NIPS2019_9710} and the objective functions turns out to be:

\vspace{-2em}
\begin{equation}
    \label{eq:maximizing-variance}
    % \begin{split}
        arg \max_{ \{ C^i \}_{i \in k} } Var(P, \{ C^i \}_{i \in k})
    % \end{split}
\end{equation}

Maximizing the variance has the advantage that it turns out to equivalent to a well-studied problem about histograms, and admits a very efficient streaming algorithm. Here we are going to use $(1+ \epsilon)-$factor approximation algorithm for histogram construction given by Guha et al. \cite{10.1145/1132863.1132873}.



\vspace{3em}
\begin{algorithm}[H]
    \label{alg:pidforest}
    \caption{$PIDForest(X,t,\psi, h, k, \epsilon)$}
    \DontPrintSemicolon
    \setstretch{1.2}
    \SetAlgoLined
    \KwComplexity{Time - $O(t d \psi log(\psi))$}
    \KwInput{$X$ - input data, $t$ - number of trees, $\psi$ - subsampling size, $h$ - max depth, 
    
    $k$ - max partitions, $\epsilon$ - for histogram construction}
    \KwOutput{List of $PIDTree$}

    $Forest$ $\leftarrow$  \{
    
        \qquad \qquad trees : EmptyList,
    
        \qquad \qquad start : EmptyDictionary,
    
        \qquad \qquad end : EmptyDictionary
    
        \qquad \}

    \For {$attr$ in $X.attributes$}{
        
        $Forest.start[attr] = X[attr].min() - \delta$   \tcp*{set $\delta$ to 1e-4 for precision issues}

        $Forest.end[attr] = X[attr].max() + \delta$

    }

    \For {$i = 1$ to $t$}{
        $X' \: \leftarrow \: sampleWithoutReplacement(X,\psi)$
        
        $Forest.trees.append(PIDTree(X',0, Forest.start, Forest.end, h, k, \epsilon))$
    }

    \Return{Forest}
\end{algorithm}


\vspace{1em}
\begin{algorithm}[H]
    \label{alg:pidtree}
    \DontPrintSemicolon
    \caption{$PIDTree(X, e, start, end, h, k, \epsilon)$}
    \setstretch{1.2}
    \SetAlgoLined
    \KwComplexity{Time - $O(d \psi log(\psi))$}
    \KwInput{$X$ - input data, $e$ - current depth, $[start, \, end]$ - interval for each attribute, $h$ - max depth, $k$ - max partitions, $\epsilon$ - for histogram construction}
    \KwOutput{a $PIDTree$}

    $tree$ $\leftarrow$  \{
    
        \qquad \qquad child : EmptyList,
    
        \qquad \qquad depth : e,
    
        \qquad \qquad sparsity : (-1)
    
        \qquad \}

        $tree.cube = Cube(start, end, \&tree)$

        \tcp{\&x stands for a reference of x}
        $tree.pointset = Pointset(filter(X, tree.cube), \& tree)$ 

        \If{tree.depth $<$ h and $\vert tree.pointset \vert > 1$}{ 
            \tcp{if not a leaf node then split}
            
            $tree.child \leftarrow \: findSplit(...)$

        }
        \Else{
            $tree.sparsity \leftarrow tree.cube.logvolume - log(\vert tree.pointset.X \vert)$
        
        }

    \Return{tree}
\end{algorithm}

\vspace{1em}
\begin{algorithm}[H]
    \label{alg:cube}
    \DontPrintSemicolon
    \caption{$Cube(start, end, node)$}
    \setstretch{1.2}
    \SetAlgoLined
    \KwInput{$[start, \, end]$ - interval for each attribute, $node$ - ref. of containing PIDTree}
    \KwOutput{a $Cube$}

    $cube$ $\leftarrow$  \{
    
        \qquad \qquad node : node

        \qquad \qquad start : start,
    
        \qquad \qquad end : end,
    
        \qquad \qquad logvolume : 0 \tcp*{log of volume of subcube}

        \qquad \qquad child : EmptyList
    
        \qquad \}

    
    \For {$attr$ in $cube.start.keys()$}{ 
        \tcp{recall that cube.start is a dictionary whose keys are attributes}
    
        $cube.logvolume\, += log(cube.start.end[attr] - cube.start.start[attr])$

    }

    \Return{cube}
\end{algorithm}

\pagebreak

\vspace{1em}
\begin{algorithm}[H]
    \label{alg:Pointset}
    \DontPrintSemicolon
    \caption{$Pointset(X, node)$}
    \setstretch{1.2}
    \SetAlgoLined
    \KwInput{$X$ - input data, $node$ - to which node this set belongs to}
    \KwOutput{a $Pointset$}

    $pointset$ $\leftarrow$  \{
    
        \qquad \qquad X : X,
    
        \qquad \qquad node : node,
    
        \qquad \qquad val : EmptyDictionary,

        \qquad \qquad count : EmptyDictionary,

        \qquad \qquad gap : EmptyDictionary
    
        \qquad \}

        \tcp{val, count and gap is used for converting to histogram problem} 
        
        \tcp{refer Appendix A of \cite{NIPS2019_9710}}

    \For {$attr$ in $X.attributes$}{

        \tcp{np.unique is a function from python numpy library}

        $v, c \leftarrow$ np.unique(X[attr], return\_counts=True)
    
        $pointset.val[attr] \leftarrow \: v$
        
        $pointset.count[attr] \leftarrow \: c$

        $pointset.gap[attr] \leftarrow [0]$

        \If{$\vert v \vert > 1$}{
        
            \tcp{sum of all elements of pointset.gap for a attr is equal to }

            \tcp{start[attr] - end[attr]}

            $g \leftarrow$ List of 0's of size $\vert v \vert$

            
            $g[0] = (v[0] + v[1]) / 2 - pointset.node.cube.start[attr]$


            \tcp{negative indexing is same as python programming language}

            $g[-1] = pointset.node.cube.end[col] - (v[-1] + v[-2]) / 2$

            \For{ i in range(1,$\vert v \vert$ - 1)}{

                $g[i] = (v[i + 1] - v[i - 1]) / 2$
                
            }

            $pointset.gap[attr] \leftarrow g$

        }

    }

    \Return{pointset}
\end{algorithm}

\pagebreak

Algorithm \ref{alg:pidtree} uses $findSplit(...)$ method to partition the current subcube into disjoint child subcubes.
$findSplit(...)$ uses the $AHIST-S(...)$ procedure given by Guha et. al \cite{10.1145/1132863.1132873} which is $(1+\epsilon)-$factor approximation of V-optimal histogram construction. Due to complexity of the $findSplit(...)$ function we have not provided it's pseudo-code. You can find my implementation of the PIDForest and AHIST-S at \href{https://github.com/KishoreKaushal/AnomalyDetection/tree/master/pidforest}{this code repository}.


Producing an anomaly score for each point is fairly straightforward. Say we want to compute a score for $y \in [0, 1]^d$. 
Each tree in the forest maps $y$ to a leaf node $v$ and gives it a score $PIDTree.sparsity$. 
We take the 70-80\% percentile score as our final score (any robust analog of the max will do).


\section{Comparison with isolation forest}

PIDForest improves upon the issues with isolation forest. Instead of choosing an attribute for partitioning a node into disjoint sets, PIDForest uses AHIST-S method to select an attribute with higher variance. Instead of choosing a random splitting value which is used to partition the set into two, PIDForest used AHIST-S method to get the optimal buckets which is used to do finer partition not limited to 2.

As discussed in section \ref{sec:issues-with-iforest} isolation forest struggles to give decent results for high dimensional data, on the other hand PIDForest doesn't chooses the splitting attribute with uniform probability, it gives more preference to the attribute which has more variance, therefore doesn't suffer much from higher dimension.

PIDForest is more complex data structure in comparison to Isolation Forest.

\section{Conclusion}
\label{sec:pidforest-conclusion}

PIDForest is arguably one of the best off-the-shelf algorithms for anomaly detection on a large, heterogenous dataset.  
It inherits many of the desirable features of Isolation Forests,while also improving on it in important ways.
This ends the breif discussion on PIDForest algorithm, you can refer to Vatsal et al. \cite{NIPS2019_9710} for more details.

In chapter \ref{ch:introduction} I discussed some of the major challenges face by any anomaly detection algorithm. 
Masking and swamping is well handled by the isolation forest and pidforest. 
Concept drift is a major challenge in online anomaly detection. 
Because of small time and space complexity of these methods, these methods can be easily used for online anomaly detection, i.e, we can retrain the model as the new data comes but this is a very poor way to handle the concept drift.
Another problem with these ensemble methods is poor handling of categorical attributes. We have already seen that isolation forest can't handle non-ordinal data, whereas PIDForest requires external input from domain expert.

In the next chapter I will present some modifications to these existing methods and present a general framework in which these methods can be improved to handle categorical data, concept drift and online anomaly detection.