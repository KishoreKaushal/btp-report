\chapter{Contributions}
\label{ch:contributions}

We concluded previous chapter section \ref{sec:pidforest-conclusion} by mentioning these issues:
\vspace{-0.5em}
\begin{itemize}
    \setlength\itemsep{-0.5em}
    \item Online Anomaly detection
    \item Handling categorical attributes
    \item Concept Drift
\end{itemize}

In this chapter I will present some enhancements that can be made to improve the performance of isolation forest and pidforest.


\section{Feedback guided anomaly detection}
\label{sec:feedback-guided-anomaly-detection}

Anomaly detectors are often used to produce a ranked list of statistical anomalies, which are examined by human analysts in order to extract the actual anomalies of interest. 

This can be exceedingly difficult and time consuming when most high-ranking anomalies are false positives and not interesting from an application perspective. 

Siddiqui et al. \cite{10.1145/3219819.3220083} address this problem and gives a general framework of how we can convert unsupervised anomaly detection to a semi-supervised anomaly detection problem in which a feedback is given by a domain expert which is used to improve the accuracy of the anomaly detection model. 
Feedback guided anomaly discovery can be model in online convex optimization (OCO) framework. 


\subsection{Online convex optimization}
\label{subsec:online-convex-optimization}

OCO is formulated as an iterative game against a potentially adversarial environment where our moves are vectors from a convex set $S$. At discrete time steps $t$ the game proceeds as follows:

\begin{enumerate}
    \setlength\itemsep{-0.5em}
    \item We select a vector $w_t \in S$.
    \item The environment selects a convex function $f_t:S \rightarrow R$.
    \item We suffer a loss $f_t(w_t)$.
\end{enumerate}

The goal is to select a sequence of vectors with small accumulated loss over time. 
Given, a $T$-step game episode where we play $(w_1, w_2, \dot , w_T)$ against $(f_1, f_2, \dot,f_T)$ the total $T$ step regret is equal to:

\begin{equation}
    \label{eq:regret}
    Regret_T = \sum_{t=1}^{T} f_t(w_t) - \min_{w^* \in S} \sum_{t=1}^{T} f_t(w^*)
\end{equation}

Refer chapter 2 of \cite{10.1561/2200000018} for more details.

\subsection{Modelling in OCO framework}
\label{subsec:query-guided-anomaly-discovery-as-oco}

Query-guided anomaly discovery can also be viewed as a game
where on each round we output an anomaly ranking over the data
instances and we get feedback on the top-ranked instance. We
wish to minimize the number of times we receive "nominal" as the
feedback response.

To put this problem in OCO framework Siqqiqui et al. \cite{10.1145/3219819.3220083} have put some reasonable restrictions on the form of the anomaly detectors that we will consider.
Only family of generalized linear anomaly detectors(GLADs) which are defined by i) a feature function $\phi : D \rightarrow R^n$, which maps data instances to n-dimenstional vectors and ii) n-dimenstional weight vector $w$ are considered.
In this context anomaly score for an instance $x$ is defined to be $SCORE(x;w) = - \phi \cdot w$ with larger score corresponding to more anomalous instances.



Given, a GLAD parameterization of an anomaly detector, we can now connect query-guided anomaly discovery to OCO.

On each feedback round we select a vector $w_t$ for the detector, which specifies an anomaly ranking over instances. 
We recieve feedback $y_t$ on the top ranked instance, where $y_t = +1$ if the instance is alien and $y_t = -1$ if it is nominal.

There are three choices of loss function given in the Siddqui et al. \cite{10.1145/3219819.3220083}: i) linear loss ii) log-likelihood loss iii) logistic loss. 

With the experiments, I came to conclusion that overall linear loss performs better than other two in terms of performance, computational complexity, and accuracy. Hence, throughout this report I will stick with only linear loss.

\begin{defn}
    \label{defn:linear-loss}
    (Linear loss)
    Let $x_t$ be the top-ranked instance in $D$ under the ranking given by $w_t$. The linear loss is given by 

    \vspace{-2em}
    \begin{equation}
        \label{eq:linear-loss}
        f_t(w_t) = -y_t SCORE(x_t;w_t) = y_t w_t \cdot \phi (x_t)
    \end{equation}
\end{defn}

Algorithm 1 of Siqqiqui et al. \cite{10.1145/3219819.3220083} gives a general framework in which OCO can be applied on anomaly detector methods for query-guided anomaly discovery. In the next section we will model the isolation forest and pidforest in OCO framework.


\section{Feedback guided isolation forest}

The isolation forest assigns an anomaly score to an instance $x$ based on its average isolation depth across the randomized forest, ref [\ref{alg:PathLength}]. 
In particular, the score is (a normalized version of) the negative of
this average depth.

We need to define a GLAD model that replicates isolation forest.

\textbf{Define $\phi_e(x)$} be a binary feature that is 1 if instance x goes through the edge and 0 otherwise.
\textbf{Define $w_e$} be the weight of each edge.
\textbf{Define $\phi$}  be a vector that concatenate all of the features across the forest in a consistent order.
\textbf{Define $w$}  be a vector that concatenate all of the weights across the forest in a consistent order.

Now the modified model for isolation forest is given by the following algorithms:



\vspace{1em}
\begin{algorithm}[H]
    \caption{$feedbackITree(X)$}\label{alg:feedback-guided-itree}
    \setstretch{1.2}
    \SetAlgoLined
    \KwComplexity{Time - $O(\psi^2)$, Space - $O(\psi)$}
    \KwInput{$X$ - input data}
    \KwOutput{a $feedbackITree$}

    q $\leftarrow \: RandomChoice(X.attributes)$

    p $\leftarrow \: RandomNumber(X[$splitAttr$].min(), X[$splittAttr$].max())$

    ftree $\leftarrow$ Node \{ left $\leftarrow$ None, right $\leftarrow$ None, 
    
    \qquad\qquad\qquad size $\leftarrow \: X.size$, splitAttr $\leftarrow$ q, 
    
    \qquad\qquad\qquad splitVal $\leftarrow$ p, $w \leftarrow 1$, $\theta \leftarrow 1$\}   \tcp*{$w,\theta$ for mirror descent}

    \If{X.size $>$ 1 and X[splitAttr].numUnique() $>$ 1}{
        $X_{l} \: \leftarrow  \: X.where(q < p)$

        $X_{r} \: \leftarrow  \: X.where(q \geq p)$

        ftree.left $\leftarrow \: feedbackITree(X_{l})$

        ftree.right $\leftarrow \: feedbackITree(X_{r})$
    }

    \Return{ftree}
\end{algorithm}
\vspace{1em}


\vspace{1em}
\begin{algorithm}[H]
    \caption{$unadjustedPathLength(x, T, hlim, e)$}\label{alg:unadjustedPathLength}
    \DontPrintSemicolon
    \setstretch{1.2}
    \SetAlgoLined
    \KwComplexity{Time - $O(t\psi)$, Space - $O(1)$}
    \KwInput{$x$ - input instance, $T$ - a $feedbackITree$, $hlim$ - height limit, $e$ - current path length to be initialized to zero when called first time}
    \KwOutput{path length of $x$}

    \If{ (T.right is None) and (T.left is none) and (e $\geq$ hlim)}{

        \Return $e$ \tcp*{removed the adjustment, return unadjusted path length}

    }

    $a \: \leftarrow \: T.splitAttr$

    \tcp{(e + 1) $\rightarrow$ e + (weight of the edge)}

    \If{ $x[a] < T.splitVal$}{
        \Return  $PathLength(x, T.left, hlim, e + T.w)$
    }
    \Else{
        \Return  $PathLength(x, T.right, hlim, e + T.w)$
    }
\end{algorithm}
\pagebreak


\vspace{1em}
\begin{algorithm}[H]
    \caption{$updateWeights(x, T, hlim, \eta, y)$}\label{alg:updateWeights}
    \DontPrintSemicolon
    \setstretch{1.2}
    \SetAlgoLined
    \KwComplexity{Time - $O(t\psi)$, Space - $O(1)$}
    \KwInput{$x$ - input instance, $T$ - a $feedbackITree$, $hlim$ - height limit, $\eta$ - learning rate, $y$ - feedback}
    \KwOutput{path length of $x$}

    \If{ (T.right is None) and (T.left is none) and (e $\geq$ hlim)}{

        \Return \tcp*{no child node, all weights updated}

    }

    $a \: \leftarrow \: T.splitAttr$

    \If{ $x[a] < T.splitVal$}{

        nextNode $\leftarrow T.left$
        
    }
    \Else{

        nextNode $\leftarrow T.right$
        
    }

    \tcp{mirror descent update}

    $nextNode.\theta \leftarrow nextNode.\theta - \eta * y$

    $nextNode.w \leftarrow nextNode.\theta * (nextNode.\theta \geq 0)$

    $updateWeights(x, \&nextNode, hlim, \eta, y)$
\end{algorithm}
\vspace{2em}

You can find the complete implementation of the  \href{https://github.com/KishoreKaushal/AnomalyDetection/blob/master/isolationforest/FeedbackIsolationForest.py}{feedback guided isolation forest} at my respository: \url{https://github.com/KishoreKaushal/AnomalyDetection}